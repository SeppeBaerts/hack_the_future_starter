# Agentic Ocean Explorer - Implementation Guide

## Overview

This Flutter app is an intelligent ocean exploration assistant that uses Gemini LLM and Flutter GenUI to answer questions about ocean conditions, trends, and measurements.

## Architecture

### Agent Loop (Perceive → Plan → Act → Reflect → Present)

The app follows an agentic pattern where the AI:

1. **Perceive**: Understands the user's ocean-related question
2. **Plan**: Determines which data to fetch and how to visualize it
3. **Act**: Calls ocean data tools to retrieve information
4. **Reflect**: Analyzes the data and determines the best visualization
5. **Present**: Generates UI components to display the results

### Key Components

#### 1. Ocean Data Tools (MCP-like)

The app includes four data retrieval tools that the AI can use:

- **`getOceanTemperature(region, days)`**: Retrieves temperature time series data
- **`getOceanSalinity(region, days)`**: Retrieves salinity time series data
- **`getWaveData(count)`**: Retrieves wave measurements from multiple locations
- **`getCurrentConditions(region)`**: Retrieves current ocean conditions

All tools use mock data generated by `OceanDataService` for demonstration purposes.

#### 2. Custom Ocean Widgets

Four specialized GenUI widgets for ocean data visualization:

- **OceanTemperatureCard**: Displays temperature with thermometer icon
- **WaveInfoCard**: Shows wave height, period, and direction
- **SalinityCard**: Displays salinity in PSU (Practical Salinity Units)
- **DataTrendCard**: Shows min/avg/max statistics for time series data

#### 3. Agent Log Panel

A collapsible panel that shows the agent's reasoning process in real-time:
- Color-coded by agent step (Perceive, Plan, Act, Reflect, Present)
- Timestamped entries
- Clear button to reset the log

#### 4. Stop/Abort Functionality

A stop button appears during agent processing, allowing users to interrupt the AI's workflow.

## Features

### MVP Features ✅

- [x] Chat interface for ocean-related questions
- [x] Transparent agent activity logging
- [x] Four MCP-like ocean data tools
- [x] Four custom ocean visualization widgets
- [x] Stop button during processing
- [x] Mock data fallback
- [x] Toggle to show/hide agent log

### Example Queries

Try asking the AI:

- "What is the ocean temperature in the North Sea over the past month?"
- "Show me salinity trends in the Atlantic Ocean"
- "Where were the highest waves measured?"
- "What are the current conditions in the Mediterranean?"

### Supported Ocean Regions

The mock data includes realistic values for:
- North Sea (Noordzee)
- Atlantic Ocean
- Pacific Ocean
- Mediterranean Sea
- Baltic Sea

## Code Structure

```
lib/
├── features/
│   └── chat/
│       ├── models/
│       │   ├── agent_log_entry.dart      # Agent step logging model
│       │   └── chat_message.dart         # Chat message model
│       ├── services/
│       │   ├── agent_log_service.dart    # Tracks agent activities
│       │   ├── genui_service.dart        # GenUI catalog & tools setup
│       │   └── ocean_data_service.dart   # Mock ocean data provider
│       ├── view/
│       │   └── chat_screen.dart          # Main UI with chat & log panel
│       ├── viewmodel/
│       │   └── chat_view_model.dart      # Business logic & state
│       └── widgets/
│           ├── agent_log_panel.dart      # Agent activity log UI
│           └── ocean_widgets.dart        # Custom ocean catalog items
└── main.dart                             # App entry point
```

## How It Works

1. **User asks a question** → The question is logged in the agent log
2. **AI perceives the question** → Understands what data is needed
3. **AI plans the response** → Decides which tools to call
4. **AI acts** → Calls ocean data tools (getOceanTemperature, etc.)
5. **AI reflects** → Analyzes the returned data
6. **AI presents** → Creates custom ocean widgets with the data

All steps are transparently logged in the Agent Activity Log panel.

## Extending the App

### Adding New Ocean Data Tools

Edit `lib/features/chat/services/genui_service.dart`:

```dart
DynamicAiTool(
  name: 'getYourTool',
  description: 'Description of what this tool does',
  parameters: {
    'param': PropertySchema<String>(
      description: 'Parameter description',
      defaultValue: 'default',
    ),
  },
  handler: (args) async {
    // Your implementation
    return {'result': 'data'};
  },
)
```

### Adding New Custom Widgets

Edit `lib/features/chat/widgets/ocean_widgets.dart`:

1. Create the Flutter widget
2. Define a `CatalogItem` for GenUI
3. Add it to the catalog in `genui_service.dart`

### Replacing Mock Data with Real APIs

Replace `OceanDataService` implementation in `lib/features/chat/services/ocean_data_service.dart` with real API calls to ocean data providers like:
- NOAA (National Oceanic and Atmospheric Administration)
- Copernicus Marine Service
- OpenWeather Marine API

## Firebase Setup (Required)

Before running the app:

1. Install Firebase CLI and FlutterFire CLI (see README.md)
2. Create a Firebase project
3. Run `flutterfire configure` to generate `firebase_options.dart`
4. Enable Firebase AI Logic (Gemini Developer API) in Firebase Console
5. Enable the Vertex AI API in Google Cloud Console

See the main [README.md](README.md) for detailed Firebase setup instructions.

## Running the App

```bash
flutter pub get
flutter run
```

## Troubleshooting

### "Firebase AI Logic API not enabled"
- Enable the API at: https://console.developers.google.com/apis/api/firebasevertexai.googleapis.com/overview
- Wait a few minutes for propagation

### Custom widgets not appearing
- Check that widgets are added to the catalog in `genui_service.dart`
- Verify widget properties match the PropertySchema definitions

### Agent log not showing
- Toggle the agent log visibility with the eye icon in the app bar
- Check that `agentLogService` is properly injected in ChatViewModel

## License

See repository license.
